#!/usr/bin/env python3
"""
Chromeé¡µé¢å†…å®¹è·å–å·¥å…·
ä½¿ç”¨Playwrightè·å–ç½‘é¡µçš„ä¸»è¦å†…å®¹
"""

import asyncio
import json
import sys
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional

try:
    from playwright.async_api import async_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False

class ChromeContentExtractor:
    def __init__(self):
        self.config = {
            "headless": True,
            "viewport": {"width": 1280, "height": 720},
            "timeout": 30000,
            "wait_until": "networkidle"
        }

    async def extract_content(self, url: str) -> Dict[str, Any]:
        """æå–ç½‘é¡µå†…å®¹"""
        if not PLAYWRIGHT_AVAILABLE:
            raise ImportError("Playwrightæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install playwright")

        try:
            async with async_playwright() as p:
                # å¯åŠ¨æµè§ˆå™¨
                browser = await p.chromium.launch(headless=self.config["headless"])
                page = await browser.new_page()
                
                # è®¾ç½®è§†çª—å¤§å°
                await page.set_viewport_size(self.config["viewport"])
                
                print(f"ğŸŒ æ­£åœ¨è®¿é—®: {url}")
                
                # å¯¼èˆªåˆ°é¡µé¢
                await page.goto(url, wait_until=self.config["wait_until"], timeout=self.config["timeout"])
                
                # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
                await page.wait_for_timeout(2000)
                
                # æå–é¡µé¢ä¿¡æ¯
                title = await page.title()
                
                # æå–é¡µé¢å†…å®¹
                content = await self._extract_page_content(page)
                
                # æå–å…ƒæ•°æ®
                metadata = await self._extract_metadata(page)
                
                await browser.close()
                
                return {
                    "url": url,
                    "title": title,
                    "content": content,
                    "metadata": metadata,
                    "timestamp": datetime.now().isoformat(),
                    "word_count": len(content.split()),
                    "reading_time": max(1, len(content.split()) // 200)
                }
                
        except Exception as e:
            raise Exception(f"å†…å®¹æå–å¤±è´¥: {str(e)}")

    async def _extract_page_content(self, page) -> str:
        """æå–é¡µé¢ä¸»è¦å†…å®¹"""
        content = await page.evaluate('''
            () => {
                // ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                const selectorsToRemove = [
                    'nav', 'header', 'footer', 'aside',
                    '.nav', '.navigation', '.menu', '.sidebar',
                    '.ads', '.advertisement', '.comments', '.comment-section',
                    '.social-share', '.share-buttons', '.related-posts',
                    'script', 'style', 'noscript', 'iframe'
                ];
                
                selectorsToRemove.forEach(selector => {
                    document.querySelectorAll(selector).forEach(el => el.remove());
                });
                
                // å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ
                const mainSelectors = [
                    'main', 'article', '.main', '.content', '.post',
                    '.article', '[role="main"]', '.main-content'
                ];
                
                let mainElement = null;
                for (const selector of mainSelectors) {
                    mainElement = document.querySelector(selector);
                    if (mainElement && mainElement.innerText.length > 100) {
                        break;
                    }
                }
                
                // å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸï¼Œä½¿ç”¨body
                const contentElement = mainElement || document.body;
                
                // è·å–æ–‡æœ¬å†…å®¹
                let text = contentElement.innerText || contentElement.textContent || '';
                
                // æ¸…ç†æ–‡æœ¬
                text = text
                    .replace(/\\s+/g, ' ')  // åˆå¹¶ç©ºç™½å­—ç¬¦
                    .replace(/\\n\\s*\\n\\s*\\n/g, '\\n\\n')  // ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
                    .trim();
                
                return text;
            }
        ''');
        
        return content

    async def _extract_metadata(self, page) -> Dict[str, Any]:
        """æå–é¡µé¢å…ƒæ•°æ®"""
        return await page.evaluate('''
            () => {
                const metadata = {};
                
                // æå–metaæ ‡ç­¾
                const metaTags = document.querySelectorAll('meta');
                metaTags.forEach(tag => {
                    const name = tag.getAttribute('name') || tag.getAttribute('property');
                    const content = tag.getAttribute('content');
                    if (name && content) {
                        metadata[name] = content;
                    }
                });
                
                // æå–åŸºæœ¬ä¿¡æ¯
                metadata['author'] = metadata['author'] || 
                                   document.querySelector('meta[name="author"]')?.getAttribute('content') ||
                                   document.querySelector('.author')?.innerText ||
                                   '';
                
                metadata['description'] = metadata['description'] || 
                                        document.querySelector('meta[name="description"]')?.getAttribute('content') ||
                                        '';
                
                metadata['keywords'] = metadata['keywords'] || 
                                      document.querySelector('meta[name="keywords"]')?.getAttribute('content') ||
                                      '';
                
                // æå–å‘å¸ƒæ—¶é—´
                metadata['publish_date'] = metadata['article:published_time'] || 
                                          metadata['publish_date'] ||
                                          document.querySelector('time')?.getAttribute('datetime') ||
                                          '';
                
                return metadata;
            }
        ''');

    def format_result(self, result: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–ç»“æœè¾“å‡º"""
        return f"""
ğŸ“„ é¡µé¢å†…å®¹åˆ†ææŠ¥å‘Š
============================

ğŸ”— é¡µé¢URL: {result['url']}
ğŸ“° é¡µé¢æ ‡é¢˜: {result['title']}
â° æå–æ—¶é—´: {result['timestamp']}
ğŸ“Š å†…å®¹ç»Ÿè®¡: {result['word_count']} è¯ï¼Œé¢„è®¡é˜…è¯» {result['reading_time']} åˆ†é’Ÿ

ğŸ“ é¡µé¢æè¿°: {result['metadata'].get('description', 'æ— æè¿°')}

ğŸ“ ä½œè€…: {result['metadata'].get('author', 'æœªçŸ¥ä½œè€…')}

ğŸ“… å‘å¸ƒæ—¶é—´: {result['metadata'].get('publish_date', 'æœªçŸ¥æ—¶é—´')}

ğŸ·ï¸ å…³é”®è¯: {result['metadata'].get('keywords', 'æ— å…³é”®è¯')}

ğŸ“‹ ä¸»è¦å†…å®¹:
------------------
{self._truncate_text(result['content'], 1500)}

ğŸ“ˆ å†…å®¹æ‘˜è¦:
------------------
{self._generate_summary(result['content'])}
        """.strip()

    def _truncate_text(self, text: str, max_length: int) -> str:
        """æˆªæ–­æ–‡æœ¬"""
        if len(text) <= max_length:
            return text
        return text[:max_length] + "..."

    def _generate_summary(self, text: str) -> str:
        """ç”Ÿæˆå†…å®¹æ‘˜è¦"""
        # ç®€å•çš„æ‘˜è¦ç”Ÿæˆï¼šå–å‰ä¸¤ä¸ªå¥å­
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if len(sentences) >= 2:
            summary = '. '.join(sentences[:2]) + '.'
        else:
            summary = text[:200] + (text[200:] if len(text) <= 200 else '...')
        
        return summary

    def save_result(self, result: Dict[str, Any], filename: Optional[str] = None) -> str:
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"page_content_{timestamp}.json"
        
        filepath = Path(filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        return str(filepath)

async def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("âŒ ä½¿ç”¨æ–¹æ³•: python chrome_content_extractor.py <URL>")
        print("ç¤ºä¾‹: python chrome_content_extractor.py https://example.com")
        sys.exit(1)
    
    url = sys.argv[1]
    
    # éªŒè¯URLæ ¼å¼
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    
    extractor = ChromeContentExtractor()
    
    try:
        print("ğŸš€ å¼€å§‹æå–é¡µé¢å†…å®¹...")
        result = await extractor.extract_content(url)
        
        # æ˜¾ç¤ºç»“æœ
        print(extractor.format_result(result))
        
        # ä¿å­˜ç»“æœ
        extractor.save_result(result)
        
        print("âœ… å†…å®¹æå–å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æå–å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())